{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MerveNazlim/CatNN/blob/main/SvsBsstt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lfWEOVqLGxGG"
   },
   "outputs": [],
   "source": [
    "import random as p\n",
    "\n",
    "p.seed(10)\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "#from preprocess import mkdir_p, unique_filename\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "# h5py\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(400)\n",
    "from tensorflow import keras\n",
    "# keras\n",
    "from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import models as Km\n",
    "from tensorflow.keras import layers as Kl\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler,minmax_scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold,GroupKFold\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from functools import partial\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "seed = 400\n",
    "np.random.seed(seed)\n",
    "import mplhep as hep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sso62vCKCR9c"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iKjYoU9RutdF"
   },
   "outputs": [],
   "source": [
    " def mkdir_p(path) :\n",
    "\n",
    "    import errno\n",
    "\n",
    "    \"\"\"\n",
    "    Make a directory, if it exists silence the exception\n",
    "\n",
    "    Args:\n",
    "        path : full directory path to be made\n",
    "    \"\"\"\n",
    "\n",
    "    try :\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc :\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path) :\n",
    "            pass\n",
    "        else :\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wNNxw-w7QUM7"
   },
   "outputs": [],
   "source": [
    "path_tosave = '/mnt/c/Users/aaron/Desktop/'\n",
    "file_name = 'sstt'\n",
    "path_tosave = path_tosave + file_name\n",
    "mkdir_p(path_tosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eeOlYDqIJN5R"
   },
   "outputs": [],
   "source": [
    "class Sample :\n",
    "\n",
    "    \"\"\"\n",
    "    Sample\n",
    "\n",
    "    This class will hold the feature data for a given sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name = \"\", class_label = -1, input_data = None) :\n",
    "        \"\"\"\n",
    "        Sample constructor\n",
    "\n",
    "        Args :\n",
    "            name : descriptive name of the sample (obtained from the input\n",
    "                pre-processed file)\n",
    "            input_data : numpy array of the data from the pre-processed file\n",
    "                (expects an array of dtype = np.float64, not a structured array!)\n",
    "            class_label : input class label as found in the input pre-processed\n",
    "                file\n",
    "        \"\"\"\n",
    "\n",
    "        if input_data.dtype != np.float64 :\n",
    "            raise Exception(\"ERROR Sample input data must be type 'np.float64', input is '{}'\".format(input_data.dtype))\n",
    "\n",
    "        if class_label < 0 :\n",
    "            raise ValueError(\"ERROR Sample (={})class label is not set (<0)\".format(name, class_label))\n",
    "\n",
    "        print(\"Creating sample {} (label = {})\".format(name, class_label))\n",
    "\n",
    "        self._name = name\n",
    "        self._class_label = class_label\n",
    "        self._input_data = input_data\n",
    "        self._regression_inputs = None\n",
    "\n",
    "    def name(self) :\n",
    "        return self._name\n",
    "    def class_label(self) :\n",
    "        return self._class_label\n",
    "    def data(self) :\n",
    "        return self._input_data\n",
    "    @property\n",
    "    def regression_inputs(self) :\n",
    "        return self._regression_inputs\n",
    "    @regression_inputs.setter\n",
    "    def regression_inputs(self, data) :\n",
    "        self._regression_inputs = data\n",
    "        \n",
    "\n",
    "\n",
    "class DataScaler :\n",
    "\n",
    "    \"\"\"\n",
    "    DataScaler\n",
    "\n",
    "    This class will hold the scaling information needed for the training\n",
    "    features (variables) contained in the input, pre-processed file.\n",
    "    Its constructor takes as input the scaling data dataset object\n",
    "    contained in the pre-processed file and it builds the associated\n",
    "    feature-list and an associated dictionary to store the scaling\n",
    "    parameters for each of the input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scaling_dataset = None, ignore_features = []) :\n",
    "\n",
    "        \"\"\"\n",
    "        ScalingData constructor\n",
    "\n",
    "        Args:\n",
    "            scaling_dataset : input HDF5 dataset object which contains the\n",
    "                scaling data and feature-list\n",
    "        \"\"\"\n",
    "\n",
    "        self._raw_feature_list = []\n",
    "        self._feature_list = []\n",
    "        self._scaling_dict = {}\n",
    "        self._mean = []\n",
    "        self._scale = []\n",
    "        self._var = []\n",
    "        self.load(scaling_dataset, ignore_features)\n",
    "\n",
    "    def load(self, scaling_dataset = None, ignore_features = []) :\n",
    "\n",
    "        self._raw_feature_list = list( scaling_dataset['name'] )\n",
    "        self._feature_list = list( filter( lambda x : x not in ignore_features, self._raw_feature_list ) )\n",
    "\n",
    "        #self._mean = scaling_dataset['mean']\n",
    "        #self._scale = scaling_dataset['scale']\n",
    "        #self._var = scaling_dataset['var']\n",
    "\n",
    "\n",
    "        for x in scaling_dataset :\n",
    "            name, mean, scale, var = x['name'], x['mean'], x['scale'], x['var']\n",
    "            if name in ignore_features : continue\n",
    "            self._scaling_dict[name] = { 'mean' : mean, 'scale' : scale, 'var' : var }\n",
    "            self._mean.append(mean)\n",
    "            self._scale.append(scale)\n",
    "            self._var.append(var)\n",
    "\n",
    "        self._mean = np.array(self._mean, dtype = np.float64)\n",
    "        self._scale = np.array(self._scale, dtype = np.float64)\n",
    "        self._var = np.array(self._var, dtype = np.float64)\n",
    "\n",
    "    def raw_feature_list(self) :\n",
    "        return self._raw_feature_list\n",
    "\n",
    "    def feature_list(self) :\n",
    "        return self._feature_list\n",
    "\n",
    "    def scaling_dict(self) :\n",
    "        return self._scaling_dict\n",
    "\n",
    "    def get_params(self, feature = \"\") :\n",
    "        if feature in self._scaling_dict :\n",
    "            return self._scaling_dict[feature]\n",
    "        raise KeyError(\"requested feature (={}) not found in set of scaling features\".format(feature))\n",
    "\n",
    "    def mean(self) :\n",
    "        return self._mean\n",
    "    def scale(self) :\n",
    "        return self._scale\n",
    "    def var(self) :\n",
    "        return self._var\n",
    "\n",
    "def floatify(input_array, feature_list) :\n",
    "    ftype = [(name, float) for name in feature_list]\n",
    "    return input_array.astype(ftype).view(float).reshape(input_array.shape + (-1,))\n",
    "\n",
    "def load_input_file(args) :\n",
    "\n",
    "    \"\"\"\n",
    "    Check that the provided input HDF5 file is of the expected form\n",
    "    as defined by the pre-processing. Exits if this is not the case.\n",
    "    Returns a list of the sample names found in the file.\n",
    "\n",
    "    Args :\n",
    "        args : user input to the executable\n",
    "    \"\"\"\n",
    "\n",
    "    # check that the file can be found\n",
    "    if not os.path.isfile(args) :\n",
    "        print(\"ERROR provided input file (={}) is not found or is not a regular file\".format(args))\n",
    "        sys.exit()\n",
    "\n",
    "    samples_group_name = \"samples\"\n",
    "    scaling_group_name = \"scaling\"\n",
    "    scaling_data_name = \"scaling_data\"\n",
    "\n",
    "    found_samples = False\n",
    "    found_scalings = False\n",
    "    samples = []\n",
    "    data_scaler = None\n",
    "    #features_to_ignore = [\"Mll01\",\"Mll02\",\"Mll03\",\"Mll12\",\"Mll13\",\"Mll23\"]\n",
    "    features_to_ignore = [\"DeltaR_min_lep_bjet77\",\"DeltaR_max_lep_bjet77\"]\n",
    "    #features_to_ignore = [\"\"]\n",
    "\n",
    "    with h5py.File(args, 'r') as input_file :\n",
    "\n",
    "        # look up the scalings first, in order to build the feature list used for the Sample creation\n",
    "        if scaling_group_name in input_file :\n",
    "            found_scalings = True\n",
    "            scaling_group = input_file[scaling_group_name]\n",
    "            scaling_dataset = scaling_group[scaling_data_name]\n",
    "            data_scaler = DataScaler( scaling_dataset = scaling_dataset, ignore_features = features_to_ignore )\n",
    "            print(\"DataScaler found {} features to train on (there were {} total features in the input)\".format( len(data_scaler.feature_list()), len(data_scaler.raw_feature_list() )))\n",
    "        else :\n",
    "            print(\"scaling group (={}) not found in file\".format(scaling_group_name))\n",
    "            sys.exit()\n",
    "\n",
    "        # now build the samples\n",
    "        if samples_group_name in input_file :\n",
    "            found_samples = True\n",
    "            sample_group = input_file[samples_group_name]\n",
    "            for p in sample_group :\n",
    "                process_group = sample_group[p]\n",
    "                class_label = process_group.attrs['training_label']\n",
    "                s = Sample(name = p, class_label = int(class_label),\n",
    "                    input_data = floatify( process_group['train_features'][tuple(data_scaler.feature_list())], data_scaler.feature_list() ) )\n",
    "                #print(floatify( process_group['train_features'][tuple(data_scaler.feature_list())], data_scaler.feature_list() ))\n",
    "                samples.append(s)\n",
    "\n",
    "        else :\n",
    "            print(\"samples group (={}) not found in file\".format(samples_group_name))\n",
    "            sys.exit()\n",
    "\n",
    "    samples = sorted(samples, key = lambda x: x.class_label())\n",
    "\n",
    "    return samples, data_scaler\n",
    "\n",
    "def load_input_file_val(args) :\n",
    "\n",
    "    if not os.path.isfile(args) :\n",
    "        print(\"ERROR provided input file (={}) is not found or is not a regular file\".format(args))\n",
    "        sys.exit()\n",
    "\n",
    "    samples_group_name = \"samples\"\n",
    "    scaling_group_name = \"scaling\"\n",
    "    scaling_data_name = \"scaling_data\"\n",
    "\n",
    "    samples = []\n",
    "    data_scaler = None\n",
    "\n",
    "    with h5py.File(args, 'r', libver = 'latest') as input_file :\n",
    "\n",
    "        # look up the scaling first\n",
    "        if scaling_group_name in input_file :\n",
    "            scaling_group = input_file[scaling_group_name]\n",
    "            scaling_dataset = scaling_group[scaling_data_name]\n",
    "            data_scaler = DataScaler( scaling_dataset = scaling_dataset, ignore_features = [\"DeltaR_min_lep_bjet77\",\"DeltaR_max_lep_bjet77\"] )\n",
    "            print(\"DataScaler found {} features to use as inputs (there were {} total features in the input)\".format( len(data_scaler.feature_list()), len(data_scaler.raw_feature_list())))\n",
    "        else :\n",
    "            print(\"scaling group (={}) not found in file\".format(scaling_group_name))\n",
    "            sys.exit()\n",
    "\n",
    "        # build the samples\n",
    "        if samples_group_name in input_file :\n",
    "            sample_group = input_file[samples_group_name]\n",
    "            for p in sample_group :\n",
    "                process_group = sample_group[p]\n",
    "                class_label = process_group.attrs['training_label']\n",
    "                s = Sample(name = p, class_label = int(class_label),\n",
    "                    input_data = floatify( process_group['validation_features'][tuple(data_scaler.feature_list())], data_scaler.feature_list()))\n",
    "                samples.append(s)\n",
    "        else :\n",
    "            print(\"samples group (={}) not found in file\".format(samples_group_name))\n",
    "            sys.exit()\n",
    "\n",
    "    return samples, data_scaler\n",
    "\n",
    "def build_combined_input(training_samples, data_scaler = None, scale = True) :\n",
    "\n",
    "    targets = []\n",
    "    # used extended slicing to partition arbitrary number of samples\n",
    "    sample0, sample1, *other = training_samples\n",
    "\n",
    "    targets.extend( np.ones( sample0.data().shape[0] ) * sample0.class_label() )\n",
    "    targets.extend( np.ones( sample1.data().shape[0] ) * sample1.class_label() )\n",
    "\n",
    "    inputs = np.concatenate( (sample0.data(), sample1.data()), axis = 0)\n",
    "    for sample in other :\n",
    "        inputs = np.concatenate( (inputs, sample.data()) , axis = 0 )\n",
    "        targets.extend( np.ones( sample.data().shape[0] ) * sample.class_label() )\n",
    "\n",
    "    # perform scaling\n",
    "    input_notscale = inputs\n",
    "    if scale :\n",
    "        inputs = (inputs - data_scaler.mean()) / data_scaler.scale()\n",
    "        #print(inputs)\n",
    "\n",
    "    targets = np.array(targets, dtype = int )\n",
    "\n",
    "\n",
    "    return inputs, targets, input_notscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j0Ntqz_s4mHl"
   },
   "outputs": [],
   "source": [
    "def make_nn_output_plots_oddeven( path_tosave, model = None, inputs = None, samples = None, targets = None, events=None) :\n",
    "\n",
    "    #inputs_tek = inputs[events % 2==1]\n",
    "    #inputs_cift = inputs[events % 2==0]\n",
    "    \n",
    "    ## Workaround if no eventnumbers are used:\n",
    "    # Shuffle + split into two equal size arrays\n",
    "    np.random.shuffle(inputs)\n",
    "    inputs_tek = inputs_test[0:int(inputs.shape[0]/2),:]\n",
    "    inputs_cift = inputs_test[int(inputs.shape[0]/2):int(inputs_test.shape[0]),:]\n",
    "    \n",
    "    nn_scores = np.ones([targets.size,2])\n",
    "    nn_scores_tek = model[0].predict(inputs_tek,verbose = True)\n",
    "    nn_scores_cift = model[1].predict(inputs_cift,verbose = True)\n",
    "    nn_scores[0:int(inputs.shape[0]/2),0] = nn_scores_tek.flatten()\n",
    "    nn_scores[0:int(inputs.shape[0]/2),1] = abs(nn_scores_tek - 1).flatten()\n",
    "    nn_scores[int(inputs.shape[0]/2):int(inputs.shape[0]),0] = nn_scores_cift.flatten()\n",
    "    nn_scores[int(inputs.shape[0]/2):int(inputs.shape[0]),1] = abs(nn_scores_cift - 1).flatten()\n",
    "    class_labels = set(targets)\n",
    "    targets_list = list(targets)\n",
    "    nn_scores_dict = {}\n",
    "\n",
    "    # index the sample names by their class label\n",
    "    names = {}\n",
    "    for sample in samples :\n",
    "        names[sample.class_label()] = sample.name()\n",
    "\n",
    "    # break up the predicted scores by the class label\n",
    "    for ilabel, label in enumerate(class_labels) :\n",
    "        # left-most appearance of the label\n",
    "        left = targets_list.index(label)\n",
    "        # right-most appearance of the label\n",
    "        right = len(targets_list) - 1 - targets_list[::-1].index(label)\n",
    "        nn_scores_dict[label] = nn_scores[left:right+1]\n",
    "\n",
    "    # start plotting\n",
    "    for label in class_labels :\n",
    "        #fig, ax = plt.subplots(1,1)\n",
    "        fig = plt.figure()\n",
    "        plt.grid(color='k', which='both', linestyle='--', lw=0.5, alpha=0.1, zorder = 0)\n",
    "        plt.xlabel( \"NN output for label {}\".format(names[label]), horizontalalignment='right', x=1)\n",
    "        #ax.set_xlim([1e-2,1.0])\n",
    "        plt.xlim([0,1])\n",
    "        #plt.yscale('log')\n",
    "        histargs = {\"bins\":20, \"range\":(0,1.), \"density\":True, \"histtype\":'step'}\n",
    "        #binning = np.arange(0,1,0.02)\n",
    "        #centers = (binning[1:-2] + binning[2:-1])/2\n",
    "        #ax.set_xlim((centers[0]-0.1, centers[-1]+0.1)) \n",
    "        for sample_label in nn_scores_dict :\n",
    "            sample_scores_for_label = nn_scores_dict[sample_label][:]\n",
    "            print(sample_scores_for_label)\n",
    "            #sample_weights = sample_with_label(sample_label, samples).eventweights\n",
    "\n",
    "            #yields, _ = np.histogram(sample_scores_for_label, bins = binning)\n",
    "            plt.hist(sample_scores_for_label,label = names[sample_label], **histargs)\n",
    "            #yields = yields/yields.sum()\n",
    "            #ax.step(centers, yields[1:-1], label = names[sample_label], where = 'mid')\n",
    "            \n",
    "            #ax.hist(sample_scores_for_label, bins = binning, alpha = 0.3, label = names[sample_label], density = True)\n",
    "        plt.legend(loc='best', frameon = False)\n",
    "        savename = \"nn_outputs_class_{}.pdf\".format( names[label])\n",
    "\n",
    "        savename = \"{}/{}\".format(path_tosave, savename) \n",
    "        plt.savefig(savename, bbox_inches = 'tight', dpi = 200)\n",
    "       # savename = \"nn_outputs_{}_class_{}.pdf\".format(path_tosave, names[label])\n",
    "\n",
    "     #   savename = \"{}/{}\".format(path_tosave, savename) \n",
    "     #   plt.savefig(savename, bbox_inches = 'tight', dpi = 200)\n",
    "\n",
    "    return nn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CfCXIMFm5rCV"
   },
   "outputs": [],
   "source": [
    "def make_nn_output_plots( model = None, inputs = None, samples = None, targets = None) :\n",
    "\n",
    "    # set of scores for each label: shape = (n_samples, n_outputs)\n",
    "    nn_scores = model.predict(inputs,verbose = True)\n",
    "\n",
    "    class_labels = set(targets)\n",
    "    targets_list = list(targets)\n",
    "    nn_scores_dict = {}\n",
    "\n",
    "    # index the sample names by their class label\n",
    "    names = {}\n",
    "    for sample in samples :\n",
    "        names[sample.class_label()] = sample.name()\n",
    "\n",
    "    # break up the predicted scores by the class label\n",
    "    for ilabel, label in enumerate(class_labels) :\n",
    "        # left-most appearance of the label\n",
    "        left = targets_list.index(label)\n",
    "        # right-most appearance of the label\n",
    "        right = len(targets_list) - 1 - targets_list[::-1].index(label)\n",
    "        nn_scores_dict[label] = nn_scores[left:right+1]\n",
    "\n",
    "    # start plotting\n",
    "    for label in class_labels :\n",
    "        #fig, ax = plt.subplots(1,1)\n",
    "        fig = plt.figure()\n",
    "        plt.grid(color='k', which='both', linestyle='--', lw=0.5, alpha=0.1, zorder = 0)\n",
    "        plt.xlabel( \"NN output for label {}\".format(names[label]), horizontalalignment='right', x=1)\n",
    "        #ax.set_xlim([1e-2,1.0])\n",
    "        plt.xlim([0,1])\n",
    "        #plt.yscale('log')\n",
    "        histargs = {\"bins\":20, \"range\":(0,1.), \"density\":True, \"histtype\":'step'}\n",
    "        #binning = np.arange(0,1,0.02)\n",
    "        #centers = (binning[1:-2] + binning[2:-1])/2\n",
    "        #ax.set_xlim((centers[0]-0.1, centers[-1]+0.1)) \n",
    "        for sample_label in nn_scores_dict :\n",
    "            sample_scores_for_label = nn_scores_dict[sample_label][:]\n",
    "            print(sample_scores_for_label)\n",
    "            #sample_weights = sample_with_label(sample_label, samples).eventweights\n",
    "\n",
    "            #yields, _ = np.histogram(sample_scores_for_label, bins = binning)\n",
    "            plt.hist(sample_scores_for_label,label = names[sample_label], **histargs)\n",
    "            #yields = yields/yields.sum()\n",
    "            #ax.step(centers, yields[1:-1], label = names[sample_label], where = 'mid')\n",
    "            \n",
    "            #ax.hist(sample_scores_for_label, bins = binning, alpha = 0.3, label = names[sample_label], density = True)\n",
    "        plt.legend(loc='best', frameon = False)\n",
    "        savename = \"nn_outputs_class_{}.pdf\".format( names[label])\n",
    "\n",
    "        savename = \"{}/{}\".format(path_tosave, savename) \n",
    "        plt.savefig(savename, bbox_inches = 'tight', dpi = 200)\n",
    "       # savename = \"nn_outputs_{}_class_{}.pdf\".format(path_tosave, names[label])\n",
    "\n",
    "     #   savename = \"{}/{}\".format(path_tosave, savename) \n",
    "     #   plt.savefig(savename, bbox_inches = 'tight', dpi = 200)\n",
    "\n",
    "    return nn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z7EMoV0kqBrt"
   },
   "outputs": [],
   "source": [
    "def ScaleWeights(y,w):\n",
    "    sum_wpos = sum( w[i] for i in range(len(y)) if y[i] == 1.0  )\n",
    "    sum_wneg = sum( w[i] for i in range(len(y)) if y[i] == 0.0  )\n",
    "\n",
    "    for i in range(len(w)):\n",
    "        if (y[i]==1.0):\n",
    "            w[i] = w[i] * (0.5/sum_wpos)\n",
    "        else:\n",
    "            w[i] = w[i] * (0.5/sum_wneg)\n",
    "\n",
    "    w_av = sum(w)/len(w)\n",
    "    w[:] = [x/w_av for x in w]\n",
    "     \n",
    "    sum_wpos_check = sum( w[i] for i in range(len(y)) if y[i] == 1.0  )\n",
    "    sum_wneg_check = sum( w[i] for i in range(len(y)) if y[i] == 0.0  )\n",
    "\n",
    "    print ('\\n======Weight Statistic========================================')\n",
    "    print ('Weights::        W(1)=%g, W(0)=%g' % (sum_wpos, sum_wneg))\n",
    "    print ('Scaled weights:: W(1)=%g, W(0)=%g' % (sum_wpos_check, sum_wneg_check))\n",
    "    print ('==============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SwFEXnfmNBw8"
   },
   "outputs": [],
   "source": [
    "def ScaleWeightsSignal(w,y):\n",
    "    sumi = sum( w[i] for i in range(len(y)) if y[i] == 1.0 )\n",
    "\n",
    "\n",
    "    for i in range(len(w)):\n",
    "      if (y[i]==1.0):\n",
    "        w[i] = sumi/len(w)\n",
    "      else: \n",
    "        w[i] = w[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZCXsLg0en3t",
    "outputId": "e659225e-afc2-4d0d-e666-c2ea90fe261d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataScaler found 10 features to train on (there were 10 total features in the input)\n",
      "Creating sample background (label = 0)\n",
      "Creating sample signal (label = 1)\n"
     ]
    }
   ],
   "source": [
    "training_samples, data_scaler = load_input_file('data/sstt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DZ3ejm3SffxD"
   },
   "outputs": [],
   "source": [
    "input_features, targets, inp_nonsc= build_combined_input(training_samples, data_scaler = data_scaler, scale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZmD_1WwHqUt_",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00140038, 0.00446875, 0.00443074, ..., 0.00461776, 0.0050069 ,\n",
       "       0.00346192])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2lSS\n",
    "weights=inp_nonsc[:,-1]\n",
    "weights[targets==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yXRVL3vggmHE"
   },
   "outputs": [],
   "source": [
    "evtnum = inp_nonsc[:,-1]\n",
    "#Drop Weight Coloumn\n",
    "inp_nonsc = inp_nonsc[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hKmPR0jL9afj"
   },
   "outputs": [],
   "source": [
    "targets = targets[np.where((weights<1) &(weights>0))]\n",
    "inp_nonsc = inp_nonsc[np.where((weights<1) &(weights>0))]\n",
    "evtnum =evtnum[np.where((weights<1) &(weights>0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "40z1TOgGTmU1"
   },
   "outputs": [],
   "source": [
    "weights = weights[np.where((weights<1) &(weights>0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "X4c_YDjX6aGT",
    "outputId": "af09acdc-0359-4907-ee49-2710cca897da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ed72c5890>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((weights[targets==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "XK3npbmj6c7L",
    "outputId": "a064935d-71c3-4829-a52f-d8b66e1c4569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ed67bb110>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(weights[targets==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlqR90IBqi19",
    "outputId": "7a97fb4b-7a3d-440c-a094-15b059d26d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======Weight Statistic========================================\n",
      "Weights::        W(1)=1140.91, W(0)=471.375\n",
      "Scaled weights:: W(1)=251241, W(0)=251241\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "ScaleWeights(targets,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2COC1eAYCtEZ",
    "outputId": "34cae392-575d-4980-bf2b-fd63bb9fe0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 251241, 1: 251241})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(targets)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pQXJLfesdLZ",
    "outputId": "6651420a-d5d3-4900-9808-9ffd4b6dd2d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.00000000e+00 1.00000000e+00 5.00000000e+00 ... 3.12736250e+05\n",
      "  7.47229062e+04 2.16903047e+05]\n",
      " [6.00000000e+00 5.00000000e+00 1.00000000e+00 ... 4.32814906e+05\n",
      "  1.00001992e+05 2.22915469e+05]\n",
      " [7.00000000e+00 3.00000000e+00 2.00000000e+00 ... 5.38805375e+05\n",
      "  1.02151969e+05 9.27254219e+04]\n",
      " ...\n",
      " [3.00000000e+00 1.00000000e+00 1.00000000e+00 ... 4.50074094e+05\n",
      "  4.53176133e+04 1.78305969e+05]\n",
      " [6.00000000e+00 5.00000000e+00 5.00000000e+00 ... 7.84143625e+05\n",
      "  4.57842383e+04 9.00132969e+04]\n",
      " [3.00000000e+00 1.00000000e+00 5.00000000e+00 ... 2.91216125e+05\n",
      "  1.45114094e+05 1.12973891e+05]]\n",
      "[[ 0.01876467 -1.24122022  1.10369847 ... -0.55443049 -0.29336801\n",
      "  -0.23280678]\n",
      " [ 1.09017064  0.98226258 -1.065905   ... -0.15071604 -0.17935377\n",
      "  -0.21309283]\n",
      " [ 1.62587363 -0.12947882 -0.52350413 ...  0.20563275 -0.1696569\n",
      "  -0.63996913]\n",
      " ...\n",
      " [-0.51693832 -1.24122022 -1.065905   ... -0.09268921 -0.42599234\n",
      "  -0.3593616 ]\n",
      " [ 1.09017064  0.98226258  1.10369847 ...  1.03048039 -0.42388776\n",
      "  -0.64886184]\n",
      " [-0.51693832 -1.24122022  1.10369847 ... -0.62678294  0.02411172\n",
      "  -0.57357702]]\n"
     ]
    }
   ],
   "source": [
    "#scale the input between 0-1\n",
    "#print(inp_nonsc)\n",
    "scaler = StandardScaler().fit(inp_nonsc)\n",
    "print(inp_nonsc)\n",
    "inp_nonsc = scaler.transform(inp_nonsc)\n",
    "print(inp_nonsc)\n",
    "#inp_nonsc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "D64hk_18vSe1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "scalerfile = path_tosave+'/scaler_standard.bin'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_bc7302zXGpX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aTZWhtxRUSVG"
   },
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch): \n",
    "  return 0.05 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UTFalfU-YlbL"
   },
   "outputs": [],
   "source": [
    "def lr_step_decay(epoch) : #initial_lr, drop, epoch_to_drop) :\n",
    "\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.9\n",
    "    epoch_to_drop = 3\n",
    "\n",
    "    if epoch >= 50 :\n",
    "        epoch == 50\n",
    "    elif epoch >= 25 :\n",
    "        epoch -= 25\n",
    "#        new_lr = initial_lr\n",
    "        print('INFO Setting learning rate back to initial LR (={})'.format(initial_lr))\n",
    "\n",
    "    new_lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epoch_to_drop))\n",
    "    print('INFO LR Schedule: {}'.format(new_lr))\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nEyuBu9pz_wY"
   },
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "auc_targets = []\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "results = []\n",
    "model_history = []\n",
    "fitfull_history = []\n",
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OdVzc3rvvsRs"
   },
   "outputs": [],
   "source": [
    "    def fit_func2(n_classes, input_features_scale, targets, n_epochs = 400, batch_size = 2000, num_folds = 10) :\n",
    "        # Define the K-fold Cross Validator\n",
    "        #kfold = GroupKFold(n_splits=1)\n",
    "        kfold = StratifiedKFold(n_splits=2)\n",
    "        # Define per-fold score containers\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        number = int((targets.size)/2)\n",
    "        #group = [0,1]*number\n",
    "        group = evtnum % 2 ==0\n",
    "        targets_encoded=targets\n",
    "        # K-fold Cross Validation model evaluation\n",
    "        fold_no = 1\n",
    "        for train, test in kfold.split(input_features_scale, targets_encoded,group):\n",
    "            print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "            layer_opts = dict( activation = 'sigmoid', kernel_initializer = initializers.glorot_normal(seed=seed))\n",
    "            input_layer = Kl.Input(shape = (inp_nonsc.shape[1],) )\n",
    "            x = Kl.Dense( 36, **layer_opts) (input_layer)\n",
    "            x = Kl.Dropout(0.4)(x)\n",
    "            x = Kl.Dense( 48, **layer_opts) (x)\n",
    "            y_pred = Kl.Dense( 1., activation = 'sigmoid', name = \"OutputLayer\" )(x)\n",
    "            model = Km.Model(inputs= input_layer, outputs=y_pred )\n",
    "            model_optimizer = Adam(lr=0.0001)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(),loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "            model.summary() \n",
    "            lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "            lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_step_decay)\n",
    "            # Generate a print\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ...')\n",
    "              # Fit data to model\n",
    "            train_X, test_X = input_features_scale[train], input_features_scale[test]\n",
    "            train_y, test_y = targets_encoded[train], targets_encoded[test]\n",
    "            w_train = weights[train]\n",
    "            train_0, train_1 = len(train_y[train_y==0]), len(train_y[train_y==1])\n",
    "            test_0, test_1 = len(test_y[test_y==0]), len(test_y[test_y==1])\n",
    "            print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "            fit_history = model.fit(train_X, train_y, epochs = n_epochs, shuffle = True, batch_size = batch_size ,validation_data = (test_X,test_y), sample_weight=w_train,callbacks=[tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20, verbose = True, min_delta = 0.001),lr_schedule]) #\n",
    "            scores = model.evaluate(test_X, test_y,batch_size=batch_size, verbose=0)\n",
    "            nn_scores = model.predict(test_X,verbose = True)\n",
    "            print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1] * 100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "            results.append(scores)\n",
    "            model_history.append(model)\n",
    "            fitfull_history.append(fit_history)\n",
    "            auc_scores.append(nn_scores)\n",
    "            auc_targets.append(targets[test])\n",
    "\n",
    "              # Increase fold number\n",
    "            fold_no = fold_no + 1\n",
    "        #fit_history = model1.fit(x_train, y_train, epochs = n_epochs, batch_size = batch_size, shuffle = True, validation_data = (x_val, y_val),callbacks=callback)\n",
    "\n",
    "        #self._fit_history = self._model.fit(x_train, y_train, epochs = n_epochs, batch_size = batch_size, shuffle = True, validation_data = (x_val, y_val), callbacks = callbacks)\n",
    "        return model_history, fitfull_history, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "sUfVYkFYfck7",
    "outputId": "72fe31b2-8daa-42f5-e7ec-8220e300f1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [125621 125622 125623 ... 502479 502480 502481] TEST: [     0      1      2 ... 376858 376859 376860]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 36)                360       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                1776      \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 2,185\n",
      "Trainable params: 2,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      ">Train: 0=125620, 1=125621, Test: 0=125621, 1=125620\n",
      "Train on 251241 samples, validate on 251241 samples\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 1/70\n",
      "251241/251241 [==============================] - 0s 2us/sample - loss: 0.3756 - accuracy: 0.8463 - val_loss: 0.2821 - val_accuracy: 0.8845\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 2/70\n",
      " 54000/251241 [=====>........................] - ETA: 0s - loss: 0.3152 - accuracy: 0.8803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avdgraaf/.local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3140 - accuracy: 0.8803 - val_loss: 0.2756 - val_accuracy: 0.8868\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 3/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3063 - accuracy: 0.8827 - val_loss: 0.2792 - val_accuracy: 0.8850\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 4/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3021 - accuracy: 0.8836 - val_loss: 0.2702 - val_accuracy: 0.8880\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 5/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2985 - accuracy: 0.8847 - val_loss: 0.2714 - val_accuracy: 0.8877\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 6/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2971 - accuracy: 0.8851 - val_loss: 0.2694 - val_accuracy: 0.8882\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 7/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2964 - accuracy: 0.8853 - val_loss: 0.2673 - val_accuracy: 0.8892\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 8/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2942 - accuracy: 0.8860 - val_loss: 0.2659 - val_accuracy: 0.8896\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 9/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2931 - accuracy: 0.8867 - val_loss: 0.2653 - val_accuracy: 0.8894\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 10/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2931 - accuracy: 0.8869 - val_loss: 0.2652 - val_accuracy: 0.8895\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 11/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2916 - accuracy: 0.8870 - val_loss: 0.2642 - val_accuracy: 0.8898\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 12/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2909 - accuracy: 0.8871 - val_loss: 0.2656 - val_accuracy: 0.8897\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 13/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2903 - accuracy: 0.8873 - val_loss: 0.2636 - val_accuracy: 0.8904\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 14/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2894 - accuracy: 0.8877 - val_loss: 0.2617 - val_accuracy: 0.8907\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 15/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2898 - accuracy: 0.8875 - val_loss: 0.2624 - val_accuracy: 0.8907\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 16/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2882 - accuracy: 0.8882 - val_loss: 0.2612 - val_accuracy: 0.8913\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 17/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2881 - accuracy: 0.8881 - val_loss: 0.2623 - val_accuracy: 0.8910\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 18/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2874 - accuracy: 0.8887 - val_loss: 0.2627 - val_accuracy: 0.8909\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 19/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2873 - accuracy: 0.8885 - val_loss: 0.2624 - val_accuracy: 0.8911\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 20/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2870 - accuracy: 0.8888 - val_loss: 0.2609 - val_accuracy: 0.8920\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 21/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2864 - accuracy: 0.8891 - val_loss: 0.2602 - val_accuracy: 0.8921\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 22/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2856 - accuracy: 0.8889 - val_loss: 0.2627 - val_accuracy: 0.8907\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 23/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2859 - accuracy: 0.8898 - val_loss: 0.2602 - val_accuracy: 0.8923\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 24/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2849 - accuracy: 0.8901 - val_loss: 0.2599 - val_accuracy: 0.8923\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 25/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2851 - accuracy: 0.8898 - val_loss: 0.2610 - val_accuracy: 0.8920\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 26/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2851 - accuracy: 0.8895 - val_loss: 0.2595 - val_accuracy: 0.8926\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 27/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2861 - accuracy: 0.8901 - val_loss: 0.2596 - val_accuracy: 0.8927\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 28/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2844 - accuracy: 0.8902 - val_loss: 0.2598 - val_accuracy: 0.8928\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 29/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2843 - accuracy: 0.8902 - val_loss: 0.2580 - val_accuracy: 0.8931\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 30/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2838 - accuracy: 0.8901 - val_loss: 0.2588 - val_accuracy: 0.8928\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 31/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2836 - accuracy: 0.8904 - val_loss: 0.2594 - val_accuracy: 0.8924\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 32/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2835 - accuracy: 0.8909 - val_loss: 0.2575 - val_accuracy: 0.8937\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 33/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2825 - accuracy: 0.8913 - val_loss: 0.2576 - val_accuracy: 0.8933\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 34/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2825 - accuracy: 0.8910 - val_loss: 0.2591 - val_accuracy: 0.8929\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 35/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2827 - accuracy: 0.8910 - val_loss: 0.2583 - val_accuracy: 0.8933\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 36/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2817 - accuracy: 0.8911 - val_loss: 0.2566 - val_accuracy: 0.8937\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 37/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2819 - accuracy: 0.8908 - val_loss: 0.2573 - val_accuracy: 0.8937\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 38/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2817 - accuracy: 0.8914 - val_loss: 0.2574 - val_accuracy: 0.8935\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 39/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2817 - accuracy: 0.8915 - val_loss: 0.2577 - val_accuracy: 0.8940\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 40/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2811 - accuracy: 0.8911 - val_loss: 0.2577 - val_accuracy: 0.8935\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 41/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2814 - accuracy: 0.8914 - val_loss: 0.2591 - val_accuracy: 0.8940\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 42/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2814 - accuracy: 0.8915 - val_loss: 0.2582 - val_accuracy: 0.8940\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 43/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2808 - accuracy: 0.8914 - val_loss: 0.2580 - val_accuracy: 0.8938\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 44/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2814 - accuracy: 0.8918 - val_loss: 0.2582 - val_accuracy: 0.8941\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 45/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2800 - accuracy: 0.8918 - val_loss: 0.2589 - val_accuracy: 0.8931\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 46/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2805 - accuracy: 0.8915 - val_loss: 0.2583 - val_accuracy: 0.8935\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 47/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2802 - accuracy: 0.8920 - val_loss: 0.2572 - val_accuracy: 0.8942\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 48/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2804 - accuracy: 0.8922 - val_loss: 0.2574 - val_accuracy: 0.8938\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 49/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2801 - accuracy: 0.8921 - val_loss: 0.2594 - val_accuracy: 0.8938\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 50/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2803 - accuracy: 0.8921 - val_loss: 0.2573 - val_accuracy: 0.8935\n",
      "INFO LR Schedule: 0.0016677181699666576\n",
      "Epoch 51/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2793 - accuracy: 0.8924 - val_loss: 0.2581 - val_accuracy: 0.8937\n",
      "INFO LR Schedule: 0.0016677181699666576\n",
      "Epoch 52/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2791 - accuracy: 0.8920 - val_loss: 0.2576 - val_accuracy: 0.8939\n",
      "INFO LR Schedule: 0.0016677181699666576\n",
      "Epoch 53/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2799 - accuracy: 0.8923 - val_loss: 0.2579 - val_accuracy: 0.8941\n",
      "INFO LR Schedule: 0.0015009463529699917\n",
      "Epoch 54/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2792 - accuracy: 0.8919 - val_loss: 0.2575 - val_accuracy: 0.8939\n",
      "INFO LR Schedule: 0.0015009463529699917\n",
      "Epoch 55/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2792 - accuracy: 0.8920 - val_loss: 0.2584 - val_accuracy: 0.8941\n",
      "INFO LR Schedule: 0.0015009463529699917\n",
      "Epoch 56/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2791 - accuracy: 0.8926 - val_loss: 0.2584 - val_accuracy: 0.8942\n",
      "Epoch 00056: early stopping\n",
      "Score for fold 1: loss of 0.2584213929337066; accuracy of 89.42131400108337%\n",
      "TRAIN: [     0      1      2 ... 376858 376859 376860] TEST: [125621 125622 125623 ... 502479 502480 502481]\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 36)                360       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 48)                1776      \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 2,185\n",
      "Trainable params: 2,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      ">Train: 0=125621, 1=125620, Test: 0=125620, 1=125621\n",
      "Train on 251241 samples, validate on 251241 samples\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 1/70\n",
      "251241/251241 [==============================] - 0s 2us/sample - loss: 0.3884 - accuracy: 0.8345 - val_loss: 0.2806 - val_accuracy: 0.8855\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 2/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3166 - accuracy: 0.8791 - val_loss: 0.2767 - val_accuracy: 0.8869\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 3/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3084 - accuracy: 0.8816 - val_loss: 0.2723 - val_accuracy: 0.8882\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 4/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3039 - accuracy: 0.8833 - val_loss: 0.2709 - val_accuracy: 0.8878\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 5/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.3011 - accuracy: 0.8844 - val_loss: 0.2697 - val_accuracy: 0.8880\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 6/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2988 - accuracy: 0.8847 - val_loss: 0.2668 - val_accuracy: 0.8893\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 7/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2966 - accuracy: 0.8853 - val_loss: 0.2654 - val_accuracy: 0.8896\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 8/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2953 - accuracy: 0.8858 - val_loss: 0.2671 - val_accuracy: 0.8892\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 9/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2944 - accuracy: 0.8864 - val_loss: 0.2676 - val_accuracy: 0.8889\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 10/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2936 - accuracy: 0.8871 - val_loss: 0.2640 - val_accuracy: 0.8905\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 11/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2930 - accuracy: 0.8874 - val_loss: 0.2625 - val_accuracy: 0.8906\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 12/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2917 - accuracy: 0.8878 - val_loss: 0.2626 - val_accuracy: 0.8908\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 13/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2909 - accuracy: 0.8877 - val_loss: 0.2636 - val_accuracy: 0.8907\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 14/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2903 - accuracy: 0.8877 - val_loss: 0.2622 - val_accuracy: 0.8911\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 15/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2900 - accuracy: 0.8880 - val_loss: 0.2620 - val_accuracy: 0.8914\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 16/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2895 - accuracy: 0.8879 - val_loss: 0.2614 - val_accuracy: 0.8912\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 17/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2888 - accuracy: 0.8889 - val_loss: 0.2625 - val_accuracy: 0.8904\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 18/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2885 - accuracy: 0.8888 - val_loss: 0.2605 - val_accuracy: 0.8921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 19/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2876 - accuracy: 0.8887 - val_loss: 0.2602 - val_accuracy: 0.8919\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 20/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2873 - accuracy: 0.8888 - val_loss: 0.2608 - val_accuracy: 0.8920\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 21/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2878 - accuracy: 0.8889 - val_loss: 0.2609 - val_accuracy: 0.8912\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 22/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2866 - accuracy: 0.8896 - val_loss: 0.2596 - val_accuracy: 0.8920\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 23/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2860 - accuracy: 0.8894 - val_loss: 0.2583 - val_accuracy: 0.8924\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 24/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2865 - accuracy: 0.8896 - val_loss: 0.2598 - val_accuracy: 0.8919\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 25/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2862 - accuracy: 0.8895 - val_loss: 0.2594 - val_accuracy: 0.8917\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 26/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2870 - accuracy: 0.8892 - val_loss: 0.2583 - val_accuracy: 0.8932\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.01\n",
      "Epoch 27/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2861 - accuracy: 0.8897 - val_loss: 0.2581 - val_accuracy: 0.8926\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 28/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2854 - accuracy: 0.8898 - val_loss: 0.2609 - val_accuracy: 0.8917\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 29/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2850 - accuracy: 0.8905 - val_loss: 0.2596 - val_accuracy: 0.8935\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.009000000000000001\n",
      "Epoch 30/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2843 - accuracy: 0.8907 - val_loss: 0.2624 - val_accuracy: 0.8913\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 31/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2847 - accuracy: 0.8901 - val_loss: 0.2554 - val_accuracy: 0.8941\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 32/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2849 - accuracy: 0.8901 - val_loss: 0.2563 - val_accuracy: 0.8936\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.008100000000000001\n",
      "Epoch 33/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2841 - accuracy: 0.8907 - val_loss: 0.2565 - val_accuracy: 0.8938\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 34/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2834 - accuracy: 0.8903 - val_loss: 0.2572 - val_accuracy: 0.8933\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 35/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2829 - accuracy: 0.8911 - val_loss: 0.2570 - val_accuracy: 0.8935\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.007290000000000001\n",
      "Epoch 36/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2835 - accuracy: 0.8909 - val_loss: 0.2588 - val_accuracy: 0.8922\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 37/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2826 - accuracy: 0.8907 - val_loss: 0.2571 - val_accuracy: 0.8933\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 38/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2830 - accuracy: 0.8910 - val_loss: 0.2582 - val_accuracy: 0.8938\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.006561\n",
      "Epoch 39/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2832 - accuracy: 0.8904 - val_loss: 0.2569 - val_accuracy: 0.8943\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 40/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2824 - accuracy: 0.8910 - val_loss: 0.2597 - val_accuracy: 0.8929\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 41/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2820 - accuracy: 0.8912 - val_loss: 0.2575 - val_accuracy: 0.8942\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.005904900000000001\n",
      "Epoch 42/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2821 - accuracy: 0.8908 - val_loss: 0.2578 - val_accuracy: 0.8940\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 43/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2814 - accuracy: 0.8917 - val_loss: 0.2574 - val_accuracy: 0.8936\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 44/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2810 - accuracy: 0.8916 - val_loss: 0.2583 - val_accuracy: 0.8945\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.00531441\n",
      "Epoch 45/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2811 - accuracy: 0.8914 - val_loss: 0.2572 - val_accuracy: 0.8941\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 46/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2812 - accuracy: 0.8912 - val_loss: 0.2571 - val_accuracy: 0.8944\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 47/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2813 - accuracy: 0.8911 - val_loss: 0.2579 - val_accuracy: 0.8934\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004782969000000001\n",
      "Epoch 48/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2813 - accuracy: 0.8913 - val_loss: 0.2571 - val_accuracy: 0.8942\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 49/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2806 - accuracy: 0.8914 - val_loss: 0.2564 - val_accuracy: 0.8943\n",
      "INFO Setting learning rate back to initial LR (=0.01)\n",
      "INFO LR Schedule: 0.004304672100000001\n",
      "Epoch 50/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2803 - accuracy: 0.8919 - val_loss: 0.2577 - val_accuracy: 0.8937\n",
      "INFO LR Schedule: 0.0016677181699666576\n",
      "Epoch 51/70\n",
      "251241/251241 [==============================] - 0s 1us/sample - loss: 0.2802 - accuracy: 0.8914 - val_loss: 0.2576 - val_accuracy: 0.8944\n",
      "Epoch 00051: early stopping\n",
      "Score for fold 2: loss of 0.2575581383238202; accuracy of 89.44041728973389%\n"
     ]
    }
   ],
   "source": [
    "my_model_fin_func, fit_history_func,results = fit_func2( len(training_samples)-1, inp_nonsc, targets, n_epochs = 70, batch_size = 2000, num_folds = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "T-20FxuymeZj",
    "outputId": "35ee3260-a5b0-4eaa-b78c-0003244332d5"
   },
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    plt.plot(fit_history_func[x].history['loss'], label='Train. fold-'+str(x))\n",
    "    plt.plot(fit_history_func[x].history['val_loss'], label='Val. fold-'+str(x))\n",
    "    plt.title('DNN ($D_{in}$=7, $D_{hidden}$=2, 0.2Drop)')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "saveit = \"{}/{}\".format(path_tosave, \"dnn_lossepo.png\") \n",
    "plt.savefig(saveit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "nrUmodWPsH_J",
    "outputId": "9f622150-77db-47d5-a6d5-58ab567fbbf3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(0,2):\n",
    "    plt.plot(fit_history_func[x].history['accuracy'], label='Train. fold-'+str(x))\n",
    "    plt.plot(fit_history_func[x].history['val_accuracy'], label='Val. fold-'+str(x))\n",
    "    plt.title('DNN ($D_{in}$=7, $D_{hidden}$=2, 0.2Drop)')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "saveit = \"{}/{}\".format(path_tosave, \"dnn_accepo.png\") \n",
    "plt.savefig(saveit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZoElcyiKvl3",
    "outputId": "e7032ad5-b209-46ca-dfea-51e266bee5d2"
   },
   "outputs": [],
   "source": [
    "output_dir = path_tosave\n",
    "file_name = \"2hdm\"\n",
    "job_suff = \"_{}\".format(file_name)\n",
    "arch_name = \"architecture_cift{}.json\".format(job_suff)\n",
    "weights_name = \"weights_cift{}.h5\".format(job_suff)\n",
    "\n",
    "mkdir_p(output_dir)\n",
    "arch_name = \"{}/{}\".format(output_dir, arch_name)\n",
    "weights_name = \"{}/{}\".format(output_dir, weights_name)\n",
    "\n",
    "print(\"Saving architecture to: {}\".format(os.path.abspath(arch_name)))\n",
    "print(\"Saving weights to     : {}\".format(os.path.abspath(weights_name)))\n",
    "with open(arch_name, 'w') as arch_file :\n",
    "    arch_file.write(my_model_fin_func[0].to_json())\n",
    "my_model_fin_func[0].save_weights(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pHSmDQU3kfe",
    "outputId": "494f2ab7-b35d-48bd-e3d1-50bbcdc86c2b"
   },
   "outputs": [],
   "source": [
    "output_dir = path_tosave\n",
    "file_name = \"2hdm\"\n",
    "job_suff = \"_{}\".format(file_name)\n",
    "arch_name = \"architecture_tek{}.json\".format(job_suff)\n",
    "weights_name = \"weights_tek{}.h5\".format(job_suff)\n",
    "\n",
    "mkdir_p(output_dir)\n",
    "arch_name = \"{}/{}\".format(output_dir, arch_name)\n",
    "weights_name = \"{}/{}\".format(output_dir, weights_name)\n",
    "\n",
    "print(\"Saving architecture to: {}\".format(os.path.abspath(arch_name)))\n",
    "print(\"Saving weights to     : {}\".format(os.path.abspath(weights_name)))\n",
    "with open(arch_name, 'w') as arch_file :\n",
    "    arch_file.write(my_model_fin_func[1].to_json())\n",
    "my_model_fin_func[1].save_weights(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxNjLvpNL8Ey"
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "def load_model(arch_path, weights_path) :\n",
    "    #arch_name = \"/home/jovyan/test2/architecture_test2.json\"\n",
    "    #weights_name = \"/home/jovyan/test2/weights_test2.h5\"\n",
    "    print(os.path.abspath(arch_path))\n",
    "    print(\"Loading model architecture and weights (%, %)\".format(os.path.abspath(arch_path), os.path.abspath(weights_path)))\n",
    "    from tensorflow.keras.models import model_from_json\n",
    "    json_file = open(os.path.abspath(arch_path), 'r')\n",
    "    loaded_model = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model)\n",
    "    loaded_model.load_weights(os.path.abspath(weights_path))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM1fYymh4Cap",
    "outputId": "95dd63dc-b15f-4a99-e8f6-5d6838b6e28f"
   },
   "outputs": [],
   "source": [
    "arch_path = \"{}/architecture_cift_2hdm.json\".format(path_tosave)\n",
    "weights_path = \"{}/weights_cift_2hdm.h5\".format(path_tosave)\n",
    "my_model_loaded_cift = load_model(arch_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlDtNWcw4JHD",
    "outputId": "af56f23c-c0a7-486f-d3ac-dde419fdcdc4"
   },
   "outputs": [],
   "source": [
    "arch_path = \"{}/architecture_tek_2hdm.json\".format(path_tosave)\n",
    "weights_path = \"{}/weights_tek_2hdm.h5\".format(path_tosave)\n",
    "my_model_loaded_tek = load_model(arch_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_mDE9lkY5jAU",
    "outputId": "ef92fdff-3b0a-4e23-9fd0-b66e02d1b93c"
   },
   "outputs": [],
   "source": [
    "nn_scores_2 = make_nn_output_plots_oddeven( path_tosave, [my_model_loaded_cift,my_model_loaded_tek], inputs = inp_nonsc, samples = training_samples,  targets = targets, events=evtnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jkN0DGfUxVR"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(pred,truth,label):\n",
    "    fpr, tpr, thr = roc_curve(truth, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(path_tosave+\"/roc{}.png\".format(label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xH9j7ftKb9E"
   },
   "outputs": [],
   "source": [
    "nn_scores = nn_scores_2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "43TF9vxAUyRV",
    "outputId": "0629d30b-10b8-46b3-da73-45cbcadf2086"
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(nn_scores,targets,\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6cH3oqn9hLW",
    "outputId": "9fb4a4f8-8303-459e-9047-84b7daad1343"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import simps\n",
    "from numpy import trapz\n",
    "area = trapz(targets, dx=5)\n",
    "print(\"area =\", area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "Hv3Onfauzg6y",
    "outputId": "18e81ef8-38d1-489f-caea-e492bfd2d823"
   },
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=[8,6])\n",
    "color = ['blue', 'orange', 'red', 'green', 'coral',\n",
    "             'grey', 'indigo', 'gold', 'lime', 'olive',\n",
    "             'pink', 'navy', 'magenta', 'yellow', 'tomato',\n",
    "             'turquoise', 'yellowgreen', 'maroon', 'lightblue']\n",
    "\n",
    "\n",
    "fpr, tpr, thr = roc_curve(targets,nn_scores_2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color=color[0], lw=1,\n",
    "             label='ROC curve w/ dCorr train (area = {:.2f})'.format(roc_auc),linestyle='dashed')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "fpr, tpr, thr = roc_curve(targets_val,nn_scores_val)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color=color[1], lw=1,\n",
    "             label='ROC curve w/ dCorr val (area = {:.2f})'.format(roc_auc),linestyle='solid')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_1MQw0JUTJL"
   },
   "outputs": [],
   "source": [
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBVZH0xJUWat"
   },
   "outputs": [],
   "source": [
    "input_features2, targets2, inp_nonsc2= build_combined_input(training_samples, data_scaler = data_scaler, scale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "a5kDaNGgUbS0",
    "outputId": "e28eaa18-86cd-44c6-e3a8-f939e463726f"
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "bins = np.arange(0,800,10)\n",
    "bins2 = np.arange(0.0,1.0,0.1)\n",
    "f, ax = plt.subplots()\n",
    "h4 = np.array([])\n",
    "h5 = np.array([])\n",
    "h6 = np.array([])\n",
    "#for i, j in df44.iterrows(): \n",
    "#    h4 = np.append(h4, j[df4['ptnames2'][i]]/1000)\n",
    "\n",
    "#for i, j in df55.iterrows(): \n",
    "#    h5 = np.append(h5, j[df5['ptnames2'][i]]/1000)\n",
    "#print(h)\n",
    "#for i, j in df66.iterrows(): \n",
    "#    h6 = np.append(h6, j[df6['ptnames2'][i]]/1000)\n",
    "pt = inp_nonsc2[:,6]/1000\n",
    "signalpt = pt[targets==1]\n",
    "backpt = pt[targets==0]\n",
    "signal_sc = nn_scores_2[targets==1]\n",
    "back_sc = nn_scores_2[targets==0]\n",
    "h44, bins = np.histogram(signalpt, bins)\n",
    "h55, bins = np.histogram(backpt, bins)\n",
    "#h66, bins = np.histogram(h6, bins)\n",
    "h1,bins2 = np.histogram(signal_sc, bins2)\n",
    "h2,bins2 = np.histogram(back_sc, bins2)\n",
    "\n",
    "hep.histplot([h44 ,h55], bins ,label=['signal','back'], linewidth=2.0, density=True)\n",
    "\n",
    "#    print() \n",
    "#row = next(df2.iterrows())[1]\n",
    "#print(row)\n",
    "ax.legend()\n",
    "#ax.set_xlabel(\"$p_T$ of the c-jet\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "LJkMtXj0Ut_D",
    "outputId": "a4ad2a06-6fa0-4616-a3e3-8732d18999c1"
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "f1, ax1 = plt.subplots()\n",
    "h1,bins2 = np.histogram(signal_sc, bins2)\n",
    "h2,bins2 = np.histogram(back_sc, bins2)\n",
    "hep.histplot([h1 ,h2], bins2 ,label=['signal','back'], linewidth=2.0, density=True)\n",
    "ax1.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SvsBsstt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
